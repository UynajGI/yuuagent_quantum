import ast
import os
import re
from pathlib import Path

import tiktoken  # <--- æ–°å¢å¯¼å…¥

# ================= 1. è·¯å¾„é…ç½® =================
# TeNPy æºç æ ¹ç›®å½• (æ ¹æ®æ‚¨çš„ç¯å¢ƒé…ç½®)
TENPY_ROOT = Path("/share/home/jiangyuan/yuuagent_quantum/tenpy")

# è¾“å‡ºç›®å½• (å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹çš„ knowledge æ–‡ä»¶å¤¹)
SRC_KNOWLEDGE = Path(__file__).parent / "knowledge"

# æºç›®å½•æ˜ å°„
EXAMPLES_SRC = TENPY_ROOT / "examples"
TENPY_PKG = TENPY_ROOT / "tenpy"
DOC_SRC = TENPY_ROOT / "doc"

# ç›®æ ‡å­ç›®å½•
DIRS = {
    "examples": SRC_KNOWLEDGE / "examples",
    "api": SRC_KNOWLEDGE / "api",
    "tutorials": SRC_KNOWLEDGE / "tutorials",
}

# ================= 2. åŸºç¡€å·¥å…· =================


def ensure_env():
    """åˆå§‹åŒ–ç›®å½•ç»“æ„"""
    if SRC_KNOWLEDGE.exists():
        # å¯é€‰ï¼šæ¸…ç†æ—§æ•°æ®ï¼Œä¿è¯çº¯å‡€
        # shutil.rmtree(SRC_KNOWLEDGE)
        pass

    for d in DIRS.values():
        d.mkdir(parents=True, exist_ok=True)

    (SRC_KNOWLEDGE / "__init__.py").touch()
    print(f"ğŸ“‚ Knowledge Base initialized at: {SRC_KNOWLEDGE}")


def write_file(path: Path, content: str):
    """å†™å…¥æ–‡ä»¶è¾…åŠ©å‡½æ•°"""
    if not content.strip():
        return
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    # æ‰“å°ç›¸å¯¹è·¯å¾„ï¼Œä¿æŒæ—¥å¿—æ•´æ´
    print(f"  - Generated: {path.relative_to(SRC_KNOWLEDGE)}")


# ================= 3. æ¸…æ´—é€»è¾‘ï¼šExamples =================


def clean_example_code(file_path: Path) -> str:
    """æ¸…æ´—ç¤ºä¾‹ä»£ç ï¼šç§»é™¤ç»˜å›¾ã€éå¿…è¦æ‰“å°ï¼Œä¿ç•™ç‰©ç†é€»è¾‘"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
    except Exception:
        return ""  # è·³è¿‡äºŒè¿›åˆ¶æˆ–éæ–‡æœ¬æ–‡ä»¶

    cleaned_lines = [f"# Source: {file_path.name}"]

    for line in lines:
        # 1. ç§»é™¤ç»˜å›¾ç›¸å…³ (Visualizer Agent çš„å·¥ä½œ)
        if any(
            kw in line
            for kw in [
                "matplotlib",
                "pyplot",
                "plt.",
                "seaborn",
                ".show()",
                ".savefig",
                "fig, ax",
            ]
        ):
            continue

        # 2. ç§»é™¤å†—é•¿çš„æ‰“å° (å¦‚ç‰ˆæœ¬å·ã€æ¬¢è¿è¯­)
        stripped = line.strip()
        if stripped.startswith("print(") and (
            "-" * 5 in line or "version" in line or "TeNPy" in line
        ):
            continue

        cleaned_lines.append(line.rstrip())

    return "\n".join(cleaned_lines)


def process_examples():
    print("\nğŸ”¹ Processing Examples...")
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .py æ–‡ä»¶
    all_examples = list(EXAMPLES_SRC.rglob("*.py"))

    for ex in all_examples:
        # è·³è¿‡æµ‹è¯•å’Œé…ç½®æ–‡ä»¶
        if "test" in ex.name or "conftest" in ex.name:
            continue

        content = clean_example_code(ex)
        if len(content) < 50:
            continue  # å¿½ç•¥å¤ªçŸ­çš„æ–‡ä»¶

        # å‘½åç­–ç•¥ï¼šæ‰å¹³åŒ–å¤„ç†ï¼Œé˜²æ­¢åŒåå†²çª
        # ä¾‹å¦‚: userguide/d_dmrg.py -> userguide_d_dmrg.txt
        if ex.parent != EXAMPLES_SRC:
            safe_name = f"{ex.parent.name}_{ex.name}"
        else:
            safe_name = ex.name

        target = DIRS["examples"] / safe_name.replace(".py", ".txt")
        write_file(target, content)


# ================= 4. æ¸…æ´—é€»è¾‘ï¼šAPI (ASTè§£æ - å¢å¼ºç‰ˆ) =================


class APIVisitor(ast.NodeVisitor):
    """AST è®¿é—®è€…ï¼šæå–ç±»ã€å¸¦æœ‰é»˜è®¤å€¼çš„ç­¾åå’Œæ–‡æ¡£æ‘˜è¦"""

    def __init__(self):
        self.output = []
        self.current_class = None

    def _format_arg(self, arg):
        """è¾…åŠ©å‡½æ•°ï¼šå¤„ç†å¸¦ç±»å‹æ³¨è§£çš„å‚æ•°"""
        if arg.annotation:
            try:
                # Python 3.9+ æ”¯æŒ ast.unparse
                ann = ast.unparse(arg.annotation)
                return f"{arg.arg}: {ann}"
            except AttributeError:
                pass
        return arg.arg

    def _get_args_str(self, args_node):
        """è¾…åŠ©å‡½æ•°ï¼šé‡å»ºå¸¦æœ‰é»˜è®¤å€¼çš„å‚æ•°åˆ—è¡¨"""
        args = []
        defaults = args_node.defaults
        n_args = len(args_node.args)
        n_defaults = len(defaults)

        # å¤„ç†ä½ç½®å‚æ•°
        for i, arg in enumerate(args_node.args):
            arg_str = self._format_arg(arg)
            # æ£€æŸ¥æ˜¯å¦æœ‰é»˜è®¤å€¼
            default_idx = i - (n_args - n_defaults)
            if default_idx >= 0:
                try:
                    default_val = ast.unparse(defaults[default_idx])
                    arg_str += f"={default_val}"
                except AttributeError:
                    arg_str += "=..."  # Fallback for complex defaults
            args.append(arg_str)

        # å¤„ç†å…³é”®å­—å‚æ•° (kwonlyargs)
        for i, arg in enumerate(args_node.kwonlyargs):
            arg_str = self._format_arg(arg)
            if i < len(args_node.kw_defaults) and args_node.kw_defaults[i] is not None:
                try:
                    default_val = ast.unparse(args_node.kw_defaults[i])
                    arg_str += f"={default_val}"
                except AttributeError:
                    arg_str += "=..."
            args.append(arg_str)

        if args_node.vararg:
            args.append(f"*{args_node.vararg.arg}")
        if args_node.kwarg:
            args.append(f"**{args_node.kwarg.arg}")

        return ", ".join(args)

    def visit_ClassDef(self, node):
        if node.name.startswith("_"):
            return  # è·³è¿‡ç§æœ‰ç±»
        self.current_class = node.name

        doc = ast.get_docstring(node)
        doc_sum = doc.split("\n")[0] if doc else ""

        self.output.append(f"\nclass {node.name}:")
        if doc_sum:
            self.output.append(f'    """{doc_sum}"""')

        self.generic_visit(node)
        self.current_class = None

    def visit_FunctionDef(self, node):
        # è·³è¿‡ç§æœ‰æ–¹æ³•/å‡½æ•°
        if self.current_class:
            if node.name.startswith("_") and node.name != "__init__":
                return
        else:
            if node.name.startswith("_"):
                return

        # 1. æå–å®Œæ•´çš„å‡½æ•°ç­¾å (å¸¦é»˜è®¤å€¼)
        arg_str = self._get_args_str(node.args)

        # 2. æå–æ–‡æ¡£æ‘˜è¦
        doc = ast.get_docstring(node)
        doc_sum = doc.split("\n")[0] if doc else ""

        # 3. æ ¼å¼åŒ–è¾“å‡º
        indent = "    " if self.current_class else ""
        if not doc_sum:
            self.output.append(f"{indent}def {node.name}({arg_str}): pass")
        else:
            self.output.append(f"{indent}def {node.name}({arg_str}):")
            self.output.append(f'{indent}    """{doc_sum}"""')


def extract_api_from_file(file_path: Path) -> str:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read())
    except Exception:
        return ""

    visitor = APIVisitor()
    visitor.visit(tree)
    return "\n".join(visitor.output)


def process_api():
    print("\nğŸ”¹ Processing API (Source Code)...")
    # éå† tenpy åŒ…ä¸‹æ‰€æœ‰å­ç›®å½•
    for root, _, files in os.walk(TENPY_PKG):
        rel_path = Path(root).relative_to(TENPY_PKG)

        # è·³è¿‡æµ‹è¯•ç›®å½•å’Œç¼“å­˜ç›®å½•
        if "test" in str(rel_path) or "__" in str(rel_path):
            continue

        py_files = [f for f in files if f.endswith(".py") and not f.startswith("test")]
        if not py_files:
            continue

        module_content = []
        for pf in py_files:
            content = extract_api_from_file(Path(root) / pf)
            if content.strip():
                module_content.append(f"# Module: tenpy.{rel_path}.{pf[:-3]}")
                module_content.append(content)

        if module_content:
            # ç”Ÿæˆæ–‡ä»¶åï¼šå°†è·¯å¾„æ–œæ è½¬æ¢ä¸ºä¸‹åˆ’çº¿
            safe_name = str(rel_path).replace("/", "_")
            if safe_name == ".":
                safe_name = "core"

            write_file(DIRS["api"] / f"{safe_name}.txt", "\n\n".join(module_content))


# ================= 5. æ¸…æ´—é€»è¾‘ï¼šTutorials (RST) =================


def clean_rst_content(file_path: Path) -> str:
    """æ¸…æ´— RST æ–‡æ¡£ï¼šå»é™¤ Sphinx æŒ‡ä»¤ï¼Œä¿ç•™æ–‡æœ¬ã€ä»£ç å—å’Œå…¬å¼"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()
    except Exception:
        return ""

    # 1. ç®€åŒ–å¼•ç”¨é“¾æ¥ :class:`~tenpy.models.Model` -> Model
    text = re.sub(r":[a-zA-Z0-9_-]+:`~?([a-zA-Z0-9_.-]+)`", r"\1", text)

    lines = text.splitlines()
    cleaned = []

    for line in lines:
        stripped = line.strip()
        # ç§»é™¤ Sphinx æŒ‡ä»¤è¡Œ (.. xxxx::) ä½†ä¿ç•™ math (å¦‚æœ math ç´§è·Ÿå†…å®¹é€šå¸¸åœ¨ä¸‹ä¸€è¡Œç¼©è¿›)
        if stripped.startswith("..") and "::" in stripped:
            if "math::" in stripped:
                continue  # math å†…å®¹åœ¨ä¸‹ä¸€è¡Œï¼Œä¿ç•™
            continue  # å…¶ä»–æŒ‡ä»¤è·³è¿‡

        # ç§»é™¤å¼•ç”¨å®šä¹‰å’Œç´¢å¼•
        if stripped.startswith(".. _") or "toctree::" in stripped:
            continue

        cleaned.append(line)

    return "\n".join(cleaned)


def process_tutorials():
    print("\nğŸ”¹ Processing Tutorials (Documentation)...")
    all_rst = list(DOC_SRC.rglob("*.rst"))

    # 1. åƒåœ¾è¯é»‘åå• (æ–‡ä»¶ååŒ¹é…)
    IGNORE_KEYWORDS = [
        "release",
        "history",
        "what",
        "news",
        "upgrade",
        "index",
        "bib",
        "ref",
        "literat",
        "paper",
        "author",
        "credit",
        "license",
        "copyright",
        "install",
        "contribut",
        "ack",
        "todo",
        "trouble",
        "faq",
        "main",
        "changelog",
        "pip",
        "conda",
        "from_source",
        "test",
        "updating",
        "extra",
        "base",
        "class",
        "module",
        "build_doc",
        "logging",
        "overview",
        "introductions",
        "guidelines",
    ]

    categories = {"intro": [], "models": [], "algorithms": [], "advanced": []}

    for rst in all_rst:
        lower_name = rst.name.lower()
        lower_path = str(rst).lower()

        # --- è¿‡æ»¤é€»è¾‘ ---

        # 1. è·¯å¾„é»‘åå•ï¼šå¦‚æœæ–‡ä»¶åœ¨ changelog æ–‡ä»¶å¤¹é‡Œï¼Œç›´æ¥æ‰”æ‰
        if "changelog" in lower_path:
            # print(f"  [Skipped Path] {rst.relative_to(DOC_SRC)}")
            continue

        # 2. æ–‡ä»¶åé»‘åå•
        if any(bad in lower_name for bad in IGNORE_KEYWORDS):
            continue

        content = clean_rst_content(rst)
        if len(content) < 100:
            continue

        formatted = f"\n\n--- DOC: {rst.stem} ---\n{content}"

        # --- åˆ†ç±»é€»è¾‘ ---
        if "model" in lower_name:
            categories["models"].append(formatted)
        elif any(
            k in lower_name
            for k in ["algorithm", "dmrg", "tdvp", "tebd", "vumps", "contract"]
        ):
            categories["algorithms"].append(formatted)
        elif any(
            k in lower_name
            for k in [
                "guide",
                "intro",
                "lattice",
                "mps",
                "mpo",
                "site",
                "input",
                "output",
            ]
        ):
            categories["intro"].append(formatted)
        else:
            categories["advanced"].append(formatted)

    # å†™å…¥æ–‡ä»¶
    for cat, contents in categories.items():
        if contents:
            write_file(DIRS["tutorials"] / f"{cat}.txt", "".join(contents))


# ================= 6. Token ç»Ÿè®¡é€»è¾‘ (Tiktoken) =================


def estimate_token_usage():
    print("\nğŸ“Š Precise Token Usage (via tiktoken cl100k_base)")
    print("=" * 70)
    print(f"{'Category':<15} | {'File Name':<35} | {'Tokens':>10}")
    print("-" * 70)

    total_tokens = 0
    category_tokens = {}

    try:
        # DeepSeek-V3 å…¼å®¹ cl100k_base ç¼–ç 
        enc = tiktoken.get_encoding("cl100k_base")
    except Exception as e:
        print(f"âš ï¸  Error loading tiktoken: {e}")
        return

    for root, dirs, files in os.walk(SRC_KNOWLEDGE):
        for file in files:
            if not file.endswith(".txt"):
                continue

            path = Path(root) / file
            # çˆ¶æ–‡ä»¶å¤¹åä½œä¸ºåˆ†ç±» (api, examples, tutorials)
            category = Path(root).name

            try:
                with open(path, "r", encoding="utf-8") as f:
                    content = f.read()

                    # æ ¸å¿ƒç»Ÿè®¡ï¼šä½¿ç”¨ tiktoken ç¼–ç 
                    tokens = len(enc.encode(content, disallowed_special=()))

                    total_tokens += tokens
                    category_tokens[category] = (
                        category_tokens.get(category, 0) + tokens
                    )

                    # æ‰“å°å•æ–‡ä»¶ç»Ÿè®¡ (æ–‡ä»¶åæˆªæ–­)
                    display_name = (file[:32] + "..") if len(file) > 32 else file
                    print(f"{category:<15} | {display_name:<35} | {tokens:>10}")
            except Exception:
                pass

    print("=" * 70)
    print(f"ğŸ“ˆ TOTAL EXACT TOKENS: {total_tokens}")
    print("   (DeepSeek Context Limit: 128k. Safe prompt size: < 100k)")
    print("-" * 70)
    for cat, count in category_tokens.items():
        print(f"   - {cat:<12}: {count} tokens")
    print("=" * 70)


# ================= ä¸»ç¨‹åº =================


def main():
    print("ğŸš€ Starting TeNPy Knowledge Build")
    print(f"   Source Root: {TENPY_ROOT}")

    ensure_env()
    process_examples()
    process_api()
    process_tutorials()

    # æ‰§è¡Œ Token ç»Ÿè®¡
    estimate_token_usage()

    print(f"\nâœ… Build Complete! Knowledge base ready at: {SRC_KNOWLEDGE}")


if __name__ == "__main__":
    main()
